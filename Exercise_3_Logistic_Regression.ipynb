{"cells":[{"cell_type":"markdown","metadata":{"id":"wuJzhs4GYNCr"},"source":["# Programming Exercise 3: Logistic Regression\n","\n","\n","\n","## Introduction\n","\n","In this exercise, you will implement logistic regression and apply it to two different datasets.\n","\n","All the information you need for solving this assignment is in this notebook, and all the code you will be implementing will take place within this notebook.\n","\n","Before we begin with the exercises, we need to import all libraries required for this programming exercise. Throughout the course, we will be using [`numpy`](http://www.numpy.org/) for all arrays and matrix operations, and [`matplotlib`](https://matplotlib.org/) for plotting."]},{"cell_type":"markdown","metadata":{"id":"572Szz3zYNCx"},"source":["### Section 1 Logistic Regression\n","\n","In this part of the exercise, you will build a logistic regression model to predict whether a student gets admitted into a university. Suppose that you are the administrator of a university department and\n","you want to determine each applicant’s chance of admission based on their results on two exams. You have historical data from previous applicants that you can use as a training set for logistic regression. For each training example, you have the applicant’s scores on two exams and the admissions\n","decision. Your task is to build a classification model that estimates an applicant’s probability of admission based the scores from those two exams.\n","\n","The following cell will load the data and corresponding labels:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lgmshe1OYNCz"},"outputs":[],"source":["import pandas as pd\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3Vbtwq43YNC3","outputId":"5c2ca24f-f0f5-4f13-869e-e65cffcd0b80"},"outputs":[{"name":"stdout","output_type":"stream","text":["size of samples: 100\n"]}],"source":["  students = pd.read_csv(\"ex2data1.txt\", names=[\"First_Exam\", \"Second_Exam\", \"Admission_Status\"])\n","  # type casting from pandas series and pandas dataframe to numpy array\n","  X = np.array(students[[\"First_Exam\", \"Second_Exam\"]])\n","  y = np.array(students[\"Admission_Status\"])\n","  # size of dataset\n","  number_of_samples = y.size\n","  # size of X matrix, m= rows, n= features\n","  m,n = X.shape\n","  print('size of samples: {}'.format(number_of_samples))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QJoJPpy3YNC7","outputId":"faeb62b3-6a92-4fc3-99f5-db543b6d60f2"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>First_Exam</th>\n","      <th>Second_Exam</th>\n","      <th>Admission_Status</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>34.623660</td>\n","      <td>78.024693</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>30.286711</td>\n","      <td>43.894998</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>35.847409</td>\n","      <td>72.902198</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>60.182599</td>\n","      <td>86.308552</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>79.032736</td>\n","      <td>75.344376</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   First_Exam  Second_Exam  Admission_Status\n","0   34.623660    78.024693                 0\n","1   30.286711    43.894998                 0\n","2   35.847409    72.902198                 0\n","3   60.182599    86.308552                 1\n","4   79.032736    75.344376                 1"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["students.head()"]},{"cell_type":"markdown","metadata":{"id":"O9dmFnIOYNC8"},"source":["### 1.1 Visualizing the data\n","\n","Before starting to implement any learning algorithm, it is always good to visualize the data if possible.\n","please visualize the first dataset using the package `matplotlib`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D_6f2k9tYNC9"},"outputs":[],"source":["import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sMXCCMgpYNC-"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"QJEHey_0YNDA"},"source":["### 1.2 Implementation\n","\n","#### 1.2.1 Warmup exercise: sigmoid function\n","\n","Before you start with the actual cost function, recall that the logistic regression hypothesis is defined as:\n","\n","$$ h_\\theta(x) = g(\\theta^T x)$$\n","\n","where function $g$ is the sigmoid function. The sigmoid function is defined as:\n","\n","$$g(z) = \\frac{1}{1+e^{-z}}$$.\n","\n","Your first step is to implement this function `sigmoid` so it can be\n","called by the rest of your program. When you are finished, try testing a few\n","values by calling `sigmoid(x)` in a new cell. For large positive values of `x`, the sigmoid should be close to 1, while for large negative values, the sigmoid should be close to 0. Evaluating `sigmoid(0)` should give you exactly 0.5. Your code should also work with vectors and matrices. **For a matrix, your function should perform the sigmoid function on every element.**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7WOpqf5QYNDB"},"outputs":[],"source":["def sigmoid(z):\n","    \"\"\"\n","    Compute sigmoid function given the input z.\n","\n","    Parameters\n","    ----------\n","    z : array_like\n","        The input to the sigmoid function. This can be a 1-D vector\n","        or a 2-D matrix.\n","\n","    Returns\n","    -------\n","    g : array_like\n","        The computed sigmoid function. g has the same shape as z, since\n","        the sigmoid is computed element-wise on z.\n","    \"\"\"\n","\n","    return g"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F5TRpKrwYNDD","outputId":"1cd4e847-4d32-4e41-b382-2d8201305384"},"outputs":[{"name":"stdout","output_type":"stream","text":["Actual value of sigmoid(0) should be 0.5\n","Computed value of sigmoid(0) is 0.5\n"]}],"source":["# test sigmoid function\n","z = 0\n","g = sigmoid(z)\n","\n","print(\"Actual value of sigmoid(0) should be 0.5\")\n","print(\"Computed value of sigmoid(0) is {sigmoid}\".format(sigmoid= g))"]},{"cell_type":"markdown","metadata":{"id":"XE_NEv3lYNDE"},"source":["#### 1.2.2 Cost function and gradient\n","\n","Now you will implement the cost function and gradient for logistic regression. Before proceeding we add the intercept term to X.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"72Nbj6cGYNDG"},"outputs":[],"source":["# add intercept term to martix X\n","X = np.concatenate([np.ones((len(X), 1)), X], axis= 1)\n","### make  theta_0 as the bias term"]},{"cell_type":"markdown","metadata":{"id":"12dm8zC2YNDH"},"source":["Note that the parameter $\\theta_0$ represents the bias term in the model's weights.\n","Now, complete the code for the function `cost_function` to return the cost value and define `gradient_descent` to reteun gradient. Recall that the cost function in logistic regression is\n","\n","$$ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} \\left[ -y^{(i)} \\log\\left(h_\\theta\\left( x^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - h_\\theta\\left( x^{(i)} \\right) \\right) \\right]$$\n","\n","and the gradient of the cost is a vector of the same length as $\\theta$ where the $j^{th}$\n","element (for $j = 0, 1, \\cdots , n$) is defined as follows:\n","\n","$$ \\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{m} \\sum_{i=1}^m \\left( h_\\theta \\left( x^{(i)} \\right) - y^{(i)} \\right) x_j^{(i)} $$\n","\n","Note that while this gradient looks identical to the linear regression gradient, the formula is actually different because linear and logistic regression have different definitions of $h_\\theta(x)$."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MbLJncvRYNDI"},"outputs":[],"source":["def cost_function(theta, X, y):\n","    \"\"\"\n","    Compute cost_function for logistic regression.\n","\n","    Parameters\n","    ----------\n","    theta : array_like\n","        The parameters for logistic regression. This a vector\n","        of shape (n+1, ).\n","\n","    X : array_like\n","        The input dataset of shape (m x n+1) where m is the total number\n","        of data points and n is the number of features. We assume the\n","        intercept has already been added to the input.\n","\n","    y : arra_like\n","        Labels for the input. This is a vector of shape (m, ).\n","\n","    Returns\n","    -------\n","    J : float\n","        The computed value for the cost function.\n","    \"\"\"\n","\n","\n","    return J"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mnbXuL_dYNDJ"},"outputs":[],"source":["def gradient_descent(theta, X, y):\n","    \"\"\"\n","    Compute gradient_descent for logistic regression.\n","\n","    Parameters\n","    ----------\n","    theta : array_like\n","        The parameters for logistic regression. This a vector\n","        of shape (n+1, ).\n","\n","    X : array_like\n","        The input dataset of shape (m x n+1) where m is the total number\n","        of data points and n is the number of features. We assume the\n","        intercept has already been added to the input.\n","\n","    y : arra_like\n","        Labels for the input. This is a vector of shape (m, ).\n","\n","    Returns\n","    -------\n","    grad : array_like\n","        A vector of shape (n+1, ) which is the gradient of the cost\n","        function with respect to theta, at the current values of theta.\n","    \"\"\"\n","\n","\n","    return grad"]},{"cell_type":"code","source":["### check the gradient and with theta=np.zeros(n+1)"],"metadata":{"id":"2_13jB6zaEjO"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TkoCN__pYNDK","outputId":"9905c5d4-4d77-44c9-e91e-d6e3a17a0310"},"outputs":[{"name":"stdout","output_type":"stream","text":["Shape of cost value is: 0.6931471805599452\n","Shape of grad is: (3,)\n"]}],"source":["initial_theta = np.zeros(n+1)\n","grad = gradient_descent(initial_theta, X, y)\n","cost = cost_function(initial_theta,X,y)\n","print(\"Shape of cost value is: {}\".format(cost))\n","print(\"Shape of grad is: {}\".format(grad.shape))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s8bsJw1EYNDL"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["### calculate the optimal theta using stochastic gradient deacent\n","\n","\n"],"metadata":{"id":"ytZavnq1djZ8"}},{"cell_type":"code","source":["def train_using_sgd(theta, X, y, learning_rate=0.01, epochs=100):\n","    \"\"\"\n","    Perform stochastic gradient descent to train the logistic regression model.\n","    \"\"\"\n","\n","    return theta\n","\n","# Example usage:\n","# Assuming you have X, y, and theta initialized appropriately\n","# Train the logistic regression model using stochastic gradient descent\n","theta_optimized = train_using_sgd(theta, X, y)\n"],"metadata":{"id":"IZCX99tbdsyB"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UfHQSp-7YNDQ","outputId":"afdcdf79-4668-433d-a86e-4a53aa25f2d7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Optimized theta is: [-25.16131863   0.20623159   0.20147149]\n","Optimized Cost value is: 0.2035\n"]}],"source":["print(\"Optimized theta is: {}\".format(theta_optimized))\n","optimaize_cost = cost_function(theta_optimized,X,y)\n","\n","print(\"Optimized Cost value is: {:.4f}\".format(optimaize_cost))"]},{"cell_type":"markdown","source":["### Plot the decision boundary obtained from the logistic regression model trained using gradient descent\n","\n","---\n","\n"],"metadata":{"id":"Sa89Zyq_bNSV"}},{"cell_type":"code","source":[],"metadata":{"id":"X7nZhBPQt0_l"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9LIZZSW8YNDS"},"source":["#### 1.2.4 Evaluating logistic regression\n","\n","After learning the parameters, you can use the model to predict whether a particular student will be admitted. For a student with an Exam 1 score of 53 and an Exam 2 score of 94, print the admission\n","probability."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bwlWbiQaYNDT"},"outputs":[],"source":["def predict(theta, X):\n","    \"\"\"\n","    Predict whether the label is 0 or 1 using learned logistic regression.\n","    Computes the predictions for X using a threshold at 0.5\n","    (i.e., if sigmoid(theta.T*x) >= 0.5, predict 1)\n","\n","    Parameters\n","    ----------\n","    theta : array_like\n","        Parameters for logistic regression. A vecotor of shape (n+1, ).\n","\n","    X : array_like\n","        The data to use for computing predictions. The rows is the number\n","        of points to compute predictions, and columns is the number of\n","        features.\n","\n","    Returns\n","    -------\n","    p : array_like\n","        Predictions and 0 or 1 for each row in X.\n","    \"\"\"\n","\n","    return predicted"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aJ5XmIlQYNDU"},"outputs":[],"source":["def accuracy(y_pred, y):\n","    \"\"\"\n","    check prediction accuracy with comparison between predict and actual value\n","    if two parameter(predicted and actual value) are equal diff will be 0 and\n","    otherwise will be 1. count of nonzero devided by len of diff is errors. 1 minus\n","    error is our prediction accuracy\n","\n","    Parameters\n","    ----------\n","    y_pred : array_like\n","        Predicted values that output of predict function.\n","\n","    y : array_like\n","        Actual value of data, (Labels)\n","\n","    Returns\n","    -------\n","    number : float\n","        model accuracy percentage\n","    \"\"\"\n","    return accuracy"]},{"cell_type":"markdown","metadata":{"id":"TzT1r9bRYNDV"},"source":["After you have completed the code in `predict`, we proceed to report the training accuracy of your classifier by computing the percentage of examples it got correct."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PKCiA5NBYNDV"},"outputs":[],"source":["# Predict probability for a student with score 45 on exam 1\n","# and score 85 on exam 2\n"]},{"cell_type":"markdown","source":["plot the AUC and the ROC curve on your model"],"metadata":{"id":"DKXrf4DskIr7"}},{"cell_type":"markdown","metadata":{"id":"WxyZkBEeYNDW"},"source":["\n","\n","```\n","# This is formatted as code\n","```\n","\n","## Section 2 Regularized logistic regression\n","\n","In this part of the exercise, you will implement regularized logistic regression to predict whether microchips from a fabrication plant passes quality assurance (QA). During QA, each microchip goes through various tests to ensure it is functioning correctly.\n","Suppose you are the product manager of the factory and you have the test results for some microchips on two different tests. From these two tests, you would like to determine whether the microchips should be accepted or rejected. To help you make the decision, you have a dataset of test results on past microchips, from which you can build a logistic regression model.\n","\n","First, we load the data from a CSV file:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hF8nHcddYNDX","outputId":"afe7e9f8-ced4-4623-fd9a-d3d5bed1921e"},"outputs":[{"name":"stdout","output_type":"stream","text":["size of samples: 118\n"]}],"source":["microchips= pd.read_csv(\"ex2data2.txt\", names=[\"First_Test\", \"Second_Test\", \"Status\"])\n","# type casting from pandas series and pandas dataframe to numpy array\n","X = np.array(microchips[[\"First_Test\", \"Second_Test\"]])\n","y = np.array(microchips[\"Status\"])\n","# size of dataset\n","number_of_samples = y.size\n","# size of X matrix, m= rows, n= features\n","m,n = X.shape\n","print('size of samples: {}'.format(number_of_samples))"]},{"cell_type":"markdown","metadata":{"id":"VEFb4xoBYNDY"},"source":["### 2.1 Visualize the data\n","\n","Similar to the previous parts of this exercise, where the axes are the two test scores, and the positive (y = 1, accepted) and negative (y = 0, rejected) examples are shown with\n","different markers."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fobsnaq-YNDZ"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"lZTMweFEYNDa"},"source":["The above figure shows that our dataset cannot be separated into positive and negative examples by a straight-line through the plot. Therefore, a straight-forward application of logistic regression will not perform well on this dataset since logistic regression will only be able to find a linear decision boundary.\n","\n","### 2.2 Feature mapping\n","\n","One way to fit the data better is to create more features from each data point. In the function `map_feature`, we will map the features into all polynomial terms of $x_1$ and $x_2$ up to the sixth power.\n","\n","$$ \\text{map_feature}(x) = \\begin{bmatrix} 1 & x_1 & x_2 & x_1^2 & x_1 x_2 & x_2^2 & x_1^3 & \\dots & x_1 x_2^5 & x_2^6 \\end{bmatrix}^T $$\n","\n","As a result of this mapping, our vector of two features (the scores on two QA tests) has been transformed into a 28-dimensional vector. A logistic regression classifier trained on this higher-dimension feature vector will have a more complex decision boundary and will appear nonlinear when drawn in our 2-dimensional plot.\n","While the feature mapping allows us to build a more expressive classifier, it also more susceptible to overfitting. In the next parts of the exercise, you will implement regularized logistic regression to fit the data and also see for yourself how regularization can help combat the overfitting problem.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cQwU_upAYNDa"},"outputs":[],"source":["### make the map feature function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JC9PG14jYNDb"},"outputs":[],"source":["def map_feature(X1, X2, degree=6):\n","    \"\"\"\n","    Maps the two input features to quadratic features used in the regularization exercise.\n","\n","    Returns a new feature array with more features, comprising of\n","    X1, X2, X1.^2, X2.^2, X1*X2, X1*X2.^2, etc..\n","\n","    Parameters\n","    ----------\n","    X1 : array_like\n","        A vector of shape (m, 1), containing one feature for all examples.\n","\n","    X2 : array_like\n","        A vector of shape (m, 1), containing a second feature for all examples.\n","        Inputs X1, X2 must be the same size.\n","\n","    degree: int, optional\n","        The polynomial degree.\n","\n","    Returns\n","    -------\n","    : array_like\n","        A matrix of of m rows, and columns depend on the degree of polynomial.\n","    \"\"\"\n","    return array_like\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q5qAv3_WYNDb"},"outputs":[],"source":["X = map_feature(X[:,0], X[:,1])"]},{"cell_type":"markdown","metadata":{"id":"pD0w2XM0YNDf"},"source":["### 2.3 Cost function and gradient\n","Note that the parameter $\\theta_0$ represents the bias term in the model's weights.\n","Now you will implement code to compute the cost function and gradient for regularized logistic regression. Complete the code for the function `cost_function_gradient_descent_regularized` below to return the cost and gradient.\n","\n","Recall that the regularized cost function in logistic regression is\n","\n","$$ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m \\left[ -y^{(i)}\\log \\left( h_\\theta \\left(x^{(i)} \\right) \\right) - \\left( 1 - y^{(i)} \\right) \\log \\left( 1 - h_\\theta \\left( x^{(i)} \\right) \\right) \\right] + \\frac{\\lambda}{2m} \\sum_{j=1}^n \\theta_j^2 $$\n","\n","Note that you should not regularize the parameters $\\theta_0$. The gradient of the cost function is a vector where the $j^{th}$ element is defined as follows:\n","\n","$$ \\frac{\\partial J(\\theta)}{\\partial \\theta_0} = \\frac{1}{m} \\sum_{i=1}^m \\left( h_\\theta \\left(x^{(i)}\\right) - y^{(i)} \\right) x_j^{(i)} \\qquad \\text{for } j =0 $$\n","\n","$$ \\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\left( \\frac{1}{m} \\sum_{i=1}^m \\left( h_\\theta \\left(x^{(i)}\\right) - y^{(i)} \\right) x_j^{(i)} \\right) + \\frac{\\lambda}{m}\\theta_j \\qquad \\text{for } j \\ge 1 $$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-eHcBN20YNDg"},"outputs":[],"source":["def cost_function_gradient_descent_regularized(theta, X, y, lambda_):\n","    \"\"\"\n","    Compute cost and gradient for logistic regression with regularization.\n","\n","    Parameters\n","    ----------\n","    theta : array_like\n","        Logistic regression parameters. A vector with shape (n, ). n is\n","        the number of features including any intercept. If we have mapped\n","        our initial features into polynomial features, then n is the total\n","        number of polynomial features.\n","\n","    X : array_like\n","        The data set with shape (m x n). m is the number of examples, and\n","        n is the number of features (after feature mapping).\n","\n","    y : array_like\n","        The data labels. A vector with shape (m, ).\n","\n","    lambda_ : float\n","        The regularization parameter.\n","\n","    Returns\n","    -------\n","    J : float\n","        The computed value for the regularized cost function.\n","\n","    grad : array_like\n","        A vector of shape (n, ) which is the gradient of the cost\n","        function with respect to theta, at the current values of theta.\n","    \"\"\"\n","    # Initialize some useful values\n","    m = y.size  # number of training examples\n","\n","    # You need to return the following variables correctly\n","\n","\n","    # =============================================================\n","    return J, grad"]},{"cell_type":"markdown","metadata":{"id":"FXw3P0o2YNDj"},"source":["#### 2.3.1 Learning parameters using momentum\n","\n","Similar to the previous parts, you will optimize the Regularized logistic regression using the momentum algorithm optimize the model with $\\lambda = 1$, $\\lambda = 0$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kp8TarLvYNDk"},"outputs":[],"source":["def train_using_momentum(theta, X, y, learning_rate,beta,lmbda, epochs=100):\n","    \"\"\"\n","    Perform gradient deacent with momentum to train the Regularized logistic regression model.\n","    \"\"\"\n","    pass"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WX-LA3AfYNDm","outputId":"b5945a0f-ecaa-4148-e8c8-efa82dbd182a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train Accuracy: 83.1\n","Expected accuracy (with lambda = 1): 83.1 % (approx)\n","\n"]}],"source":["# Compute accuracy on our training set\n","model_predicted = predict(theta_optimized, X)\n","\n","print(\"Train Accuracy: {:.1f}\".format((np.mean(model_predicted == y) * 100)))\n","print(\"Expected accuracy (with lambda = 1): 83.1 % (approx)\\n\")"]},{"cell_type":"code","source":[],"metadata":{"id":"7QX-5uUSnjZu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### plot the decision boundery for $\\lambda =1$\n","\n","\n"],"metadata":{"id":"ez6-vws5njuV"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"fLCxtoSuYNDn"},"outputs":[],"source":["pos = y == 1\n","neg = y == 0\n"]},{"cell_type":"code","source":[],"metadata":{"id":"49-n3Jp9nz0P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### plot the decision boundery for $\\lambda=0$\n","\n","\n"],"metadata":{"id":"y7Hwiwl1n0gw"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"fTtYgud7YNDp"},"outputs":[],"source":["pos = y == 1\n","neg = y == 0\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"udsq0xOKYNDp"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}